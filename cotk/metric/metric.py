r"""
``cotk.metrics`` provides classes and functions evaluating results of models.
It provides a fair metric for every model.
"""
import random
from itertools import chain
import multiprocessing
from multiprocessing import Pool
import numpy as np
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction
from .._utils.unordered_hash import UnorderedSha256
from .._utils.imports import DummyObject
from .._utils.metaclass import LoadClassInterface, DocStringInheritor

try:
	import torch
except ImportError as err:
	torch = DummyObject(err)
	torch.Tensor = DummyObject(err)

class MetricBase(LoadClassInterface, metaclass=DocStringInheritor):
	'''Base class for metrics.
	'''
	def __init__(self):
		self.unordered_hash = UnorderedSha256()

	def hash_relevant_data(self, data_list):
		'''Invoked by :func:`forward` or :func:`close` to hash relevant data when computing a metric.

		Arguments:
			data_list (list): relevant data organized as list.
		'''
		for item in data_list:
			self.unordered_hash.update_data(repr(item).encode())

	def hashvalue(self):
		'''Invoked by :func:`close` to return hashvalue
		'''
		return self.unordered_hash.digest()


class _PrecisionRecallMetric(MetricBase):
	r"""Base class for precision recall metrics. This is an abstract class.
	Arguments:{ARGUMENTS}
	Attributes:{ATTRIBUTES}
	"""

	ARGUMENTS = r"""
		dataloader (:class:`cotk.GenerationBase`): A language generation dataloader.
		sent_per_inst (int): The number of sentences to generate for each instance.
		reference_allvocabs_key (str):
			Reference sentences are passed to :func:`forward` by ``data[reference_allvocabs_key]``.
			Default: ``resp_allvocabs``.
		gen_key (str):
			Sentences generated by model are passed to :func:`forward` by ``data[gen_key]``.
			Default: ``gen``.
	"""

	ATTRIBUTES = r"""
			res_prefix (str): Prefix added to the front of each key
						in the result dict of `close`
	"""

	def __init__(self, dataloader, \
				 sent_per_inst, \
				 reference_allvocabs_key='resp_allvocabs', \
				 gen_key='gen'):
		super().__init__()
		self.dataloader = dataloader
		self.reference_allvocabs_key = reference_allvocabs_key
		self.gen_key = gen_key
		self.sent_per_inst = sent_per_inst
		self.prec_list = []
		self.rec_list = []
		self.res_prefix = ""

	def score(self, gen, reference):
		r'''This function is called by :func:`forward`.

		Arguments:
			gen (list): list of generated word ids.
			reference (list): list of word ids of a reference.

		Returns:
			Scalar: score \in [0, 1].
		'''
		raise NotImplementedError( \
			"This function should be implemented by subclasses.")

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				* data[reference_allvocabs_key] (list of list of list):
					Reference sentences.
					Does not contain start token (eg: ``<go>``) and end token (eg: ``<eos>``).
					Size: `[batch_size, ~sentence_num, ~word_num]`, where "~" means different sizes
					in this dimension is allowed.
				* data[gen_key] (list of list of list):
					Sentence generations model outputs, similar to data[reference_allvocabs_key].
		'''
		references = data[self.reference_allvocabs_key]
		gens = data[self.gen_key]

		if len(references) != len(gens):
			raise ValueError("Batch num is not matched.")

		for line in gens:
			if len(line) != self.sent_per_inst:
				raise ValueError(\
					"Number of sentences per instance does not equal specified sent_per_inst")

		self.hash_relevant_data(list(chain(*references)))
		for reference, gen in zip(references, gens):
			# pylint: disable=no-member
			matrix = np.zeros((len(reference), len(gen)), dtype=np.float32)
			for i, single_ref in enumerate(reference):
				for j, single_gen in enumerate(gen):
					matrix[i][j] = self.score(single_gen, single_ref)
			self.prec_list.append(float(np.sum(np.max(matrix, 0))) / len(gen))
			self.rec_list.append(float(np.sum(np.max(matrix, 1))) / len(reference))

	def close(self):
		'''Return a dict which contains:

			* **precision**: average precision.
			* **recall**: average recall.
			* **hashvalue**: hash value of reference data.
		'''
		return {'{} precision'.format(self.res_prefix): np.average(self.prec_list), \
				'{} recall'.format(self.res_prefix): np.average(self.rec_list), \
				'{} hashvalue'.format(self.res_prefix): self.hashvalue()}

class BleuPrecisionRecallMetric(_PrecisionRecallMetric):
	r'''Metric for calculating sentence BLEU precision and recall

	Arguments:
		{ARGUMENTS}
	'''

	ARGUMENTS = _PrecisionRecallMetric.ARGUMENTS + r"""
		ngram (int): Specifies BLEU-ngram."""

	def __init__(self, dataloader, \
				 ngram, \
				 sent_per_inst, \
				 reference_allvocabs_key='resp_allvocabs', \
				 gen_key='gen'):
		super().__init__(dataloader, sent_per_inst, reference_allvocabs_key, gen_key)
		if ngram not in range(1, 5):
			raise ValueError("ngram should belong to [1, 4]")
		self.ngram = ngram
		self.weights = [1 / ngram] * ngram
		self.res_prefix = 'BLEU-{}'.format(ngram)

	def score(self, gen, reference):
		r'''Score function of BLEU-ngram precision and recall.

		Arguments:
			gen (list): list of generated word ids.
			reference (list): list of word ids of a reference.

		Returns:
			Scalar: score \in [0, 1].
		'''
		return sentence_bleu([reference], gen, self.weights, SmoothingFunction().method1)

class EmbSimilarityPrecisionRecallMetric(_PrecisionRecallMetric):
	r'''Metric for calculating cosine similarity precision and recall.

	Arguments:
		{ARGUMENTS}

	'''

	ARGUMENTS = _PrecisionRecallMetric.ARGUMENTS + r"""
		embed (:class:`numpy.array`): A 2-d padded array of word embeddings.
		mode (str): Specifies the operation that computes the bag-of-word representation.
			Must be ``avg`` or ``extrema``:

			* ``avg`` : element-wise average word embeddings.
			* ``extrema`` : element-wise maximum word embeddings.
	"""

	def __init__(self, dataloader, \
				 embed, \
				 mode, \
				 sent_per_inst, \
				 reference_allvocabs_key='resp_allvocabs', \
				 gen_key='gen'):
		super().__init__(dataloader, sent_per_inst, reference_allvocabs_key, gen_key)
		if not isinstance(embed, np.ndarray) or len(np.shape(embed)) != 2:
			raise ValueError("invalid type or shape or embed.")
		if mode not in ['avg', 'extrema']:
			raise ValueError("mode should be 'avg' or 'extrema'.")
		if len(embed) != self.dataloader.vocab_size:
			raise ValueError("embed size not equal to vocab size.")
		self.embed = embed
		self.mode = mode
		self.res_prefix = '{}-bow'.format(mode)

	def score(self, gen, reference):
		r'''Score function of cosine similarity precision and recall.

		Arguments:
			gen (list): list of generated word ids.
			reference (list): list of word ids of a reference.

		Returns:
			Scalar: cosine similarity between two sentence embeddings \in [0, 1].
		'''
		gen_vec = []
		ref_vec = []
		for i in gen:
			if i < 0:
				raise ValueError("gen index out of range.")
			elif i >= self.dataloader.vocab_size:
				gen_vec.append(self.embed[self.dataloader.unk_id])
			else:
				gen_vec.append(self.embed[i])
		for i in reference:
			if i < 0:
				raise ValueError("reference index out of range.")
			elif i >= self.dataloader.vocab_size:
				ref_vec.append(self.embed[self.dataloader.unk_id])
			else:
				ref_vec.append(self.embed[i])
		if self.mode == 'avg':
			gen_embed = np.average(gen_vec, 0)
			ref_embed = np.average(ref_vec, 0)
		else:
			gen_embed = np.max(gen_vec, 0)
			ref_embed = np.max(ref_vec, 0)
		cos = np.sum(gen_embed * ref_embed) / \
			  np.sqrt(np.sum(gen_embed * gen_embed) * np.sum(ref_embed * ref_embed))
		norm = (cos + 1) / 2
		return norm

class PerplexityMetric(MetricBase):
	'''Metric for calculating perplexity.

	Arguments:
		dataloader (:class:`cotk.GenerationBase`): A language generation dataloader.
		reference_allvocabs_key (str): Reference sentences with all vocabs
			are passed to :func:`forward` by ``data[reference_allvocabs_key]``.
			Default: ``resp_allvocabs``.
		reference_len_key (str): Length of reference sentences are passed to :func:`forward`
			by ``data[reference_len_key]``. Default: ``resp_length``.
		gen_log_prob_key (str): Sentence generations model outputs of **log softmax** probability
			are passed to :func:`forward` by ``data[gen_log_prob_key]``. Default: ``gen_log_prob``.
		invalid_vocab (bool): whether ``gen_log_prob`` contains invalid vocab. Default: ``False``.
		full_check (bool): whether perform full checks on ``gen_log_prob`` to make sure the sum
			of probability is 1. Otherwise, a random check will be performed for efficiency.
			If pytorch is used, full_check is always performed and this argument will be ignored.
			Default: ``False``.
	'''
	def __init__(self, dataloader, \
					   reference_allvocabs_key="resp_allvocabs", \
					   reference_len_key="resp_length", \
					   gen_log_prob_key="gen_log_prob", \
					   invalid_vocab=False, \
					   full_check=False \
			  ):
		super().__init__()
		self.dataloader = dataloader
		self.reference_allvocabs_key = reference_allvocabs_key
		self.reference_len_key = reference_len_key
		self.gen_log_prob_key = gen_log_prob_key
		self.word_loss = 0
		self.length_sum = 0
		self.invalid_vocab = invalid_vocab
		self.full_check = full_check
		self.engine_version = "unknown" # can be 'default', 'pytorch' when first forward time

		self.resp = []
		#self.resp_length = []
		self.gen_valid_log_prob = []
		self.gen_unk_log_prob = []

	def forward(self, data):
		'''Processing a batch of data. Smoothing will be performed for invalid vocabs.
		Unknowns vocabs will be ignored. As we define in dataloader (:class:`cotk.Dataloader`),
		valid vocabs = all_vocab_list[:valid_vocab_len] and
		invalid vocabs = all_vocab_list[valid_vocab_len:].
		Unknown vocabs contain tokens that don't appear in all_vocab_list.
		Refer to the docs of dataloader (:class:`cotk.Dataloader`) for more details.

		Arguments:
			data (dict): A dict at least contains the following keys:

				* data[reference_allvocabs_key] (list or :class:`numpy.ndarray` or :class:`torch.FloatTensor`):
					Reference sentences with all vocabs
					with all vocabs. Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
					Size: `[batch_size, max_sentence_length]`.
					If torch.FloatTensor is used, the following data should also be torch.FloatTensor.
				* data[reference_len_key] (list):
					Length of Reference sentences. Contains start token (eg:``<go>``)
					and end token (eg:``<eos>``). Size: `[batch_size]`.
				* data[gen_log_prob_key] (list or :class:`numpy.ndarray` or :class:`torch.FloatTensor`):
					Sentence generations model outputs of
					**log softmax** probability. Contains end token (eg:``<eos>``), but without start token
					(eg: ``<go>``).
					Size: `[batch_size, ~gen_sentence_length, vocab_size]` for ``invalid_vocab = False``, or
					`[batch_size, ~gen_sentence_length, all_vocab_size]` for ``invalid_vocab = True``,
					where "~" means different sizes in this dimension is allowed.
					If torch.FloatTensor is used, the following data should also be torch.FloatTensor.

		Warning:
			``data[gen_log_prob_key]`` must be processed after log_softmax. That means,
			``np.sum(np.exp(gen_log_prob), -1)`` equals ``np.ones((batch_size, gen_sentence_length))``
		'''
		resp_allvocabs = data[self.reference_allvocabs_key]
		resp_length = data[self.reference_len_key]
		gen_log_prob = data[self.gen_log_prob_key]

		if not isinstance(resp_allvocabs, (torch.Tensor, np.ndarray, list)):
			raise TypeError("Unkown type for resp_allvocabs")
		if not isinstance(gen_log_prob, (torch.Tensor, np.ndarray, list)):
			raise TypeError("Unkown type for gen_log_prob")
		if not isinstance(resp_length, (list, np.ndarray)):
			raise TypeError("Unkown type for resp_length")

		if self.engine_version == "unknown":
			if isinstance(resp_allvocabs, torch.Tensor):
				self.engine_version = "pytorch"
			else:
				self.engine_version = "normal"

		if (not (self.engine_version == "pytorch") == isinstance(resp_allvocabs, torch.Tensor)) or\
			(not (self.engine_version == "pytorch") == isinstance(gen_log_prob, torch.Tensor)):
			raise TypeError("If you want to use pytorch, `resp_allvocabs` and `gen_log_prob` \
				should be torch.Tensor. It can't mix with list or numpy.ndarray.")

		if self.engine_version == "pytorch":
			with torch.no_grad():
				self._pytorch_forward(resp_allvocabs, resp_length, gen_log_prob)
		else:
			self._normal_forward(resp_allvocabs, resp_length, gen_log_prob)

	def _normal_forward(self, resp_allvocabs, resp_length, gen_log_prob):
		if len(resp_allvocabs) != len(resp_length) or len(resp_allvocabs) != len(gen_log_prob):
			raise ValueError("Batch num of arguments is not matched.")

		# perform random check to assert the probability is valid
		checkid = random.randint(0, len(resp_length)-1)
		if resp_length[checkid] < 2:
			raise ValueError("resp_length must no less than 2, because <go> and <eos> are always included.")
		checkrow = random.randint(0, resp_length[checkid]-2)

		random_check_expsum = np.sum(np.exp(gen_log_prob[checkid][checkrow]))
		if not np.isclose(random_check_expsum, 1):
			raise ValueError("data[gen_log_prob_key] must be processed after log_softmax. \
				gen_log_prob[%d][%d] exp sum is equal to %f." % (checkid, checkrow, \
				random_check_expsum))

		relevant_data = []
		for i, resp_len in enumerate(resp_length):
			if resp_len < 2:
				raise ValueError("resp_length must no less than 2, because <go> and <eos> are always included.")

			resp_now = np.array(resp_allvocabs[i][1:resp_len])
			gen_now = np.array(gen_log_prob[i])
			relevant_data.append(resp_now.tolist())

			if len(resp_now.shape) != 1:
				raise ValueError("resp_allvocabs need to be 2 dimension")
			if len(gen_now.shape) != 2:
				raise ValueError("gen_log_prob need to be 3 dimension")

			# perform full check to assert the probability is valid
			if self.full_check:
				expsum = np.sum(np.exp(gen_now[:resp_len-1]), -1)
				if not np.allclose(expsum, [1] * (resp_len - 1)):
					raise ValueError("data[gen_log_prob_key] must be processed after log_softmax.")

			if not self.invalid_vocab:
				if gen_now.shape[1] != self.dataloader.vocab_size:
					raise ValueError("The third dimension gen_log_prob should be equals to vocab_size when \
						invalid_vocab = False, \
						but %d != %d" % (gen_now.shape[1], self.dataloader.vocab_size))
			else:
				if gen_now.shape[1] != self.dataloader.all_vocab_size:
					raise ValueError("The third dimension gen_log_prob should be equals to all_vocab_size \
						when invalid_vocab = True, \
						but %d != %d" % (gen_now.shape[1], self.dataloader.all_vocab_size))

			resp = resp_now
			self.resp.append(resp)
			#self.resp_length.append(resp_len)

			resp_known = resp.copy()
			if not self.invalid_vocab:
				resp_known[resp_known >= self.dataloader.vocab_size] = self.dataloader.unk_id

			self.gen_valid_log_prob.append(gen_now[list(range(resp_len-1)), resp_known])
			self.gen_unk_log_prob.append(gen_now[:resp_len-1, self.dataloader.unk_id])

		self.hash_relevant_data(relevant_data)

	def _pytorch_forward(self, resp_allvocabs, resp_length, gen_log_prob):
		if len(resp_allvocabs) != len(resp_length) or len(resp_allvocabs) != len(gen_log_prob):
			raise ValueError("Batch num of arguments is not matched.")
		if len(resp_allvocabs.shape) != 2:
			raise ValueError("resp_allvocabs need to be 2 dimension")
		if len(gen_log_prob.shape) != 3:
			raise ValueError("gen_log_prob need to be 3 dimension")

		relevant_data = []
		for i, resp_len in enumerate(resp_length):
			if resp_len < 2:
				raise ValueError("resp_length must no less than 2, because <go> and <eos> are always included.")

			resp_now = resp_allvocabs[i, 1:resp_len]
			gen_now = gen_log_prob[i, :resp_len - 1]
			relevant_data.append(resp_now.tolist())

			# perform full check to assert the probability is valid
			expsum = gen_now.exp().sum(-1)
			if not expsum.allclose(torch.ones_like(expsum)):
				raise ValueError("data[gen_log_prob_key] must be processed after log_softmax.")

			if not self.invalid_vocab:
				if gen_now.shape[1] != self.dataloader.vocab_size:
					raise ValueError("The third dimension gen_log_prob should be equals to vocab_size when \
						invalid_vocab = False, \
						but %d != %d" % (gen_now.shape[1], self.dataloader.vocab_size))
			else:
				if gen_now.shape[1] != self.dataloader.all_vocab_size:
					raise ValueError("The third dimension gen_log_prob should be equals to all_vocab_size \
						when invalid_vocab = True, \
						but %d != %d" % (gen_now.shape[1], self.dataloader.all_vocab_size))

			resp_known = resp_now.clone()
			if not self.invalid_vocab:
				resp_known[resp_known >= self.dataloader.vocab_size] = self.dataloader.unk_id

			unk_id = self.dataloader.unk_id
			vocab_size = self.dataloader.vocab_size
			invalid_vocab_size = self.dataloader.all_vocab_size - vocab_size

			# calc normal vocab
			normal_mask = ((resp_now != unk_id) & (resp_now < vocab_size)).float()
			word_loss = -(gen_now.gather(-1, resp_known.unsqueeze(1))[:, 0] * normal_mask).sum()
			length_sum = normal_mask.sum()
			# calc invalid vocab
			# smoothing from unk
			invalid_mask = (resp_now >= vocab_size).float()
			invalid_log_prob = (gen_now[:, unk_id] - \
						(torch.ones_like(gen_now[:, unk_id]) * invalid_vocab_size).log()) * invalid_mask

			if self.invalid_vocab:
				extra_invalid_log_prob = gen_now.gather(-1, resp_now.unsqueeze(1))[:, 0] * invalid_mask
				word_loss -= ((invalid_log_prob.exp() + extra_invalid_log_prob.exp()).log() \
						* invalid_mask).sum()
			else:
				word_loss -= invalid_log_prob.sum()

			length_sum += invalid_mask.sum()

			self.word_loss += word_loss.tolist()
			self.length_sum += length_sum.tolist()

		self.hash_relevant_data(relevant_data)

	@classmethod
	def run_f(cls, ele):
		'''Auxiliary function for computing perplexity:

		Returns:

			* tuple: sum of log perplexity and sum of sentence length.
		'''
		valid_log_prob, unk_log_prob, resp_now, \
				invalid_vocab, vocab_size, all_vocab_size, unk_id = ele

		# calc normal vocab
		normal_idx = np.where(np.logical_and(resp_now != unk_id, \
								resp_now < vocab_size))
		word_loss = -np.sum(valid_log_prob[normal_idx])
		length_sum = np.array(normal_idx).shape[1]
		# calc invalid vocab
		# smoothing from unk
		invalid_idx = np.where(resp_now >= vocab_size)
		invalid_log_prob = unk_log_prob[invalid_idx] - np.log(all_vocab_size - vocab_size)
		if invalid_vocab:
			extra_invalid_log_prob = valid_log_prob[invalid_idx]
			word_loss -= np.sum(np.log( \
					np.exp(invalid_log_prob) + np.exp(extra_invalid_log_prob) \
				))
		else:
			word_loss -= np.sum(invalid_log_prob)
		length_sum += np.array(invalid_idx).shape[1]

		return word_loss, length_sum

	def close(self):
		'''Return a dict which contains:

			* **perplexity**: perplexity value.
			* **perplexity hashvalue**: hash value of reference data.
		'''

		if self.engine_version == "pytorch":
			# pytorch is finished when forward
			pass
		else:
			loader = self.dataloader
			tasks = ((self.gen_valid_log_prob[i], self.gen_unk_log_prob[i], self.resp[i], \
							self.invalid_vocab, loader.vocab_size, loader.all_vocab_size, loader.unk_id) \
							for i, _ in enumerate(self.gen_valid_log_prob))

			# if len(self.gen_valid_log_prob) > 100:
			# 	pool = Pool(multiprocessing.cpu_count())
			# 	for ans in tqdm.tqdm(pool.imap_unordered(self.run_f, tasks, chunksize=20), \
			# 		total=len(self.gen_valid_log_prob)):
			# 		self.word_loss += ans[0]
			# 		self.length_sum += ans[1]
			# 	pool.close()
			# 	pool.join()
			# else:
			for ans in map(self.run_f, tasks):
				self.word_loss += ans[0]
				self.length_sum += ans[1]

			self.resp = []
			self.gen_valid_log_prob = []
			self.gen_unk_log_prob = []

		print(self.word_loss)
		print(self.length_sum)


		return {"perplexity": np.exp(self.word_loss / self.length_sum), \
				"perplexity hashvalue": self.hashvalue()}

class MultiTurnPerplexityMetric(MetricBase):
	'''Metric for calculating multi-turn perplexity.

	Arguments:
		dataloader (:class:`cotk.GenerationBase`): A language generation dataloader.
		reference_allvocabs_key (str): Reference sentences with all vocabs
			are passed to :func:`forward` by ``data[reference_allvocabs_key]``.
			Default: ``sent_allvocabs``.
		reference_len_key (str): Length of reference sentences are passed to :func:`forward`
			by ``data[reference_len_key]``. Default: ``sent_length``.
		gen_log_prob_key (str): Sentence generations model outputs of **log softmax** probability
			are passed to :func:`forward` by ``data[gen_log_prob_key]``. Default: ``gen_log_prob``.
		invalid_vocab (bool): whether ``gen_log_prob`` contains invalid vocab. Default: ``False``.
		full_check (bool): whether perform full checks on ``gen_log_prob`` to make sure the sum
			of probability is 1. Otherwise, a random check will be performed for efficiency.
			Default: ``False``.
	'''
	def __init__(self, dataloader, reference_allvocabs_key="sent_allvocabs", \
					   reference_len_key="sent_length", \
					   gen_log_prob_key="gen_log_prob", \
					   invalid_vocab=False, \
					   full_check=False \
			  ):
		super().__init__()
		self.dataloader = dataloader
		self.reference_allvocabs_key = reference_allvocabs_key
		self.reference_len_key = reference_len_key
		self.gen_log_prob_key = gen_log_prob_key
		self.invalid_vocab = invalid_vocab
		self.sub_metric = PerplexityMetric(dataloader, \
				reference_allvocabs_key="sent_allvocabs", \
				reference_len_key="sent_length", \
				gen_log_prob_key="gen_log_prob", \
				invalid_vocab=invalid_vocab, \
				full_check=full_check)

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				* data[reference_allvocabs_key] (list or :class:`numpy.ndarray`):
					Reference sentences	with all vocabs.
					Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
					Size: `[batch_size, max_turn_length, max_sentence_length]`
				* data[reference_len_key] (list of list):
					Length of Reference sentences. Contains	start token (eg:``<go>``) and
					end token (eg:``<eos>``). It must **NOT** be padded,
					which means the inner lists may have different length.
					Length of outer list: `batch_size`.
				* data[gen_log_prob_key] (list or :class:`numpy.ndarray`):
					Sentence generations model outputs of **log softmax** probability.
					Contains end token (eg:``<eos>``), but without start token
					(eg: ``<go>``).
					size: `[batch_size, ~max_turn_length, ~gen_sentence_length, vocab_size]`,
					where "~" means different sizes in this dimension is allowed.

		Warning:
			``data[gen_log_prob_key]`` must be processed after log_softmax. That means,
			``np.sum(np.exp(gen_log_prob), -1)`` equals ``np.ones((batch_size, gen_sentence_length))``
		'''
		reference_allvocabs = data[self.reference_allvocabs_key]
		length = data[self.reference_len_key]
		gen_log_prob = data[self.gen_log_prob_key]
		if len(length) != len(reference_allvocabs) or len(length) != len(gen_log_prob):
			raise ValueError("Batch num is not matched.")

		for i, sent_length in enumerate(length):
			# Pass turn as batch for sub_metric, the result will be same.
			turn_length = len(sent_length)
			if len(reference_allvocabs[i]) < turn_length or len(gen_log_prob[i]) < turn_length:
				raise ValueError("Turn num is not matched.")
			self.sub_metric.forward({"sent_allvocabs": reference_allvocabs[i][:turn_length], \
					"sent_length": sent_length, \
					"gen_log_prob": gen_log_prob[i][:turn_length]})

	def close(self):
		'''Return a dict which contains:

			* **perplexity**: perplexity value.
			* **hashvalue**: hash value of reference data.
		'''
		return self.sub_metric.close()

class BleuCorpusMetric(MetricBase):
	'''Metric for calculating BLEU.

	Arguments:
		dataloader (:class:`cotk.GenerationBase`): A language generation dataloader.
		reference_allvocabs_key (str): Reference sentences with all vocabs
			are passed to :func:`forward` by ``data[reference_allvocabs_key]``.
			Default: ``resp_allvocabs``.
		gen_key (str): Sentences generated by model are passed to :func:`forward` by
			``data[gen_key]``. Default: ``gen``.
	'''
	def __init__(self, dataloader, reference_allvocabs_key="resp_allvocabs", gen_key="gen"):
		super().__init__()
		self.dataloader = dataloader
		self.reference_allvocabs_key = reference_allvocabs_key
		self.gen_key = gen_key
		self.refs = []
		self.hyps = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				* data[reference_allvocabs_key] (list or :class:`numpy.ndarray` of `int`):
					reference_allvocabs sentences.
					Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
					Size: `[batch_size, max_sentence_length]`.
				* data[gen_key] (list or :class:`numpy.ndarray` of `int`):
					Sentences generated by model. Contains end token (eg: ``<eos>``),
					but without start token (eg: ``<go>``).
					Size: `[batch_size, gen_sentence_length]`.
		'''
		gen = data[self.gen_key]
		resp = data[self.reference_allvocabs_key]
		if len(resp) != len(gen):
			raise ValueError("Batch num is not matched.")

		relevant_data = []
		for gen_sen, resp_sen in zip(gen, resp):
			self.hyps.append(self.dataloader.trim_index(gen_sen))
			reference = list(self.dataloader.trim_index(resp_sen[1:]))
			relevant_data.append(reference)
			self.refs.append([reference])
		self.hash_relevant_data(relevant_data)

	def close(self):
		'''Return a dict which contains:

			* **bleu**: bleu value.
			* **bleu hashvalue**: hash value of reference data.
		'''
		try:
			return {"bleu": \
				corpus_bleu(self.refs, self.hyps, smoothing_function=SmoothingFunction().method7), \
				"bleu hashvalue": self.hashvalue()}
		except ZeroDivisionError as _:
			raise ZeroDivisionError("Bleu smoothing divided by zero. This is a known bug of corpus_bleu, \
				usually caused when there is only one sample and the sample length is 1.")

class SelfBleuCorpusMetric(MetricBase):
	r'''Metric for calculating Self-BLEU.

	Arguments:
		gen_key (str): Sentences generated by model are passed to :func:`forward` by
			``data[gen_key]``. Default: ``gen``.
		sample (int): Number of examples sampled from the generated sentences. Default: ``1000``.
		seed (int): random seed for sampling. Default: ``1229``.
	'''
	def __init__(self, dataloader, \
		gen_key="gen", \
		sample=1000, \
		seed=1229):
		super().__init__()
		self.dataloader = dataloader
		self.gen_key = gen_key
		self.sample = sample
		self.refs = []
		self.hyps = []
		self.seed = seed

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				* data[gen_key] (list or :class:`numpy.ndarray` of `int`):
					Sentences generated by model. Contains end token (eg: ``<eos>``),
					but without start token (eg: ``<go>``).
					Size: `[batch_size, gen_sentence_length]`.
		'''
		gen = data[self.gen_key]

		for gen_sen in gen:
			self.hyps.append(self.dataloader.trim_index(gen_sen))
	def run_f(self, ele):
		'''Auxiliary function for computing sentence bleu:

		Arguments:
			ele (tuple): A tuple (`reference sentences`, `a hypothesis sentence`).

		Returns:

			* Scalar: **sentence-bleu** value.
		'''
		return sentence_bleu(ele[0], ele[1], smoothing_function=SmoothingFunction().method1)

	def close(self):
		'''Return a dict which contains:

			* **self-bleu**: self-bleu value.
		'''
		if self.sample > len(self.hyps):
			self.sample = len(self.hyps)
		random.seed(self.seed)
		random.shuffle(self.hyps)
		ref = self.hyps[:self.sample]

		bleu_irl = []
		if self.sample >= 1000:
			pool = Pool(multiprocessing.cpu_count())
			bleu_irl = pool.map(self.run_f, [(ref[:i]+ref[i+1:self.sample], ref[i]) \
								for i in range(self.sample)])
			pool.close()
			pool.join()
		elif self.sample > 1:
			for i in range(self.sample):
				bleu_irl.append(self.run_f((ref[:i]+ref[i+1:], ref[i])))
		return {"self-bleu" : 1.0 * sum(bleu_irl) / len(bleu_irl)}

class FwBwBleuCorpusMetric(MetricBase):
	r'''Metric for calculating FwBw-BLEU.

	Arguments:
		dataloader (:class:`cotk.GenerationBase`): A language generation dataloader.
		reference_test_key (str): Reference sentences with all vocabs in test data
			are passed to :func:`forward` by ``dataloader.data["test"][self.reference_test_key]``.
		gen_key (str): Sentences generated by model are passed to :func:`forward` by
			``data[gen_key]``. Default: ``gen``.
		sample (int): Number of examples sampled from the generated sentences. Default: ``1000``.
		seed (int): random seed for sampling. Default: ``1229``.
	'''
	def __init__(self, dataloader, \
			reference_test_key, \
			gen_key="gen", \
			sample=1000, \
			seed=1229):
		super().__init__()
		self.dataloader = dataloader
		self.reference_test_key = reference_test_key
		self.gen_key = gen_key
		self.sample = sample
		self.seed = seed
		self.refs = []
		self.hyps = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				* data[gen_key] (list or :class:`numpy.ndarray` of `int`):
					Sentences generated by model. Contains end token (eg: ``<eos>``),
					but without start token (eg: ``<go>``).
					Size: `[batch_size, gen_sentence_length]`.
		'''
		gen = data[self.gen_key]
		resp = self.dataloader.data["test"][self.reference_test_key]
		for gen_sen, resp_sen in zip(gen, resp):
			self.hyps.append(list(self.dataloader.trim_index(gen_sen)))
			self.refs.append(list(self.dataloader.trim_index(resp_sen[1:])))

	def run_f(self, ele):
		'''Auxiliary function for computing sentence bleu:

		Arguments:
			ele (tuple): A tuple (`reference sentences`, `a hypothesis sentence`).

		Returns:

			* Scalar: **sentence-bleu** value.
		'''
		return sentence_bleu(ele[0], ele[1], smoothing_function=SmoothingFunction().method1)

	def close(self):
		'''Return a dict which contains:

			* **fwbwbleu**: fw/bw bleu value.
			* **fw-bw-bleu hashvalue**: hash value of reference data.
		'''
		sample_hyps = self.sample if self.sample < len(self.hyps) else len(self.hyps)
		sample_refs = self.sample if self.sample < len(self.refs) else len(self.refs)

		random.seed(self.seed)
		random.shuffle(self.hyps)
		random.shuffle(self.refs)
		result = {}
		if sample_hyps >= 1000:
			pool = Pool(multiprocessing.cpu_count())
			bleu_irl_fw = pool.map(self.run_f, \
					[(self.refs, self.hyps[i]) for i in range(sample_hyps)])
			pool.close()
			pool.join()
		else:
			bleu_irl_fw = []
			for i in range(sample_hyps):
				bleu_irl_fw.append(self.run_f((self.refs, self.hyps[i])))

		if sample_refs >= 1000:
			pool = Pool(multiprocessing.cpu_count())
			bleu_irl_bw = pool.map(self.run_f, \
					[(self.hyps, self.refs[i]) for i in range(sample_refs)])
			pool.close()
			pool.join()
		else:
			bleu_irl_bw = []
			for i in range(sample_refs):
				bleu_irl_bw.append(self.run_f((self.hyps, self.refs[i])))
		fw_bleu = (1.0 * sum(bleu_irl_fw) / len(bleu_irl_fw))
		bw_bleu = (1.0 * sum(bleu_irl_bw) / len(bleu_irl_bw))
		result["fw-bleu"] = fw_bleu
		result["bw-bleu"] = bw_bleu
		result["fw-bw-bleu"] = 2.0 * bw_bleu * fw_bleu / (fw_bleu + bw_bleu)

		self.hash_relevant_data(self.refs)
		result["fw-bw-bleu hashvalue"] = self.hashvalue()
		return result

class MultiTurnBleuCorpusMetric(MetricBase):
	'''Metric for calculating multi-turn BLEU.

	Arguments:
		dataloader (:class:`cotk.GenerationBase`): A language generation dataloader.
		reference_allvocabs_key (str): Reference sentences with all vocabs are passed to
			:func:`forward` by ``data[reference_allvocabs_key]``.
			Default: ``reference_allvocabs``.
		gen_key (str): Sentences generated by model are passed to :func:`forward` by
			``data[gen_key]``. Default: ``gen``.
		turn_len_key (str): Turn length are passed to  :func:`forward` by
			``data[turn_len_key]``. Default: ``turn_length``.
	'''
	def __init__(self, dataloader, reference_allvocabs_key="reference_allvocabs", \
					gen_key="gen", \
					turn_len_key="turn_length" \
			  ):
		super().__init__()
		self.dataloader = dataloader
		self.reference_allvocabs_key = reference_allvocabs_key
		self.turn_len_key = turn_len_key
		self.gen_key = gen_key
		self.refs = []
		self.hyps = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				* data[reference_allvocabs_key] (list or :class:`numpy.ndarray`):
					Reference sentences with all vocabs.
					Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
					Size: `[batch_size, max_turn_length, max_sentence_length]`
				* data[gen_key] (list or :class:`numpy.ndarray`):
					3-d array of int. Sentences generated by model.
					Contains end token (eg: ``<eos>``), but without start token (eg: ``<go>``).
					Size: `[batch_size, ~max_turn_length, ~gen_sentence_length]`,
					where "~" means different sizes in this dimension is allowed.
				* data[turn_len_key] (list or :class:`numpy.ndarray`):
					Length of turns in each sample.
					Size: `[batch_size]`.
		'''
		reference_allvocabs = data[self.reference_allvocabs_key]
		length = data[self.turn_len_key]
		gen = data[self.gen_key]
		if len(length) != len(reference_allvocabs) or len(length) != len(gen):
			raise ValueError("Batch num is not matched.")

		for i, turn_length in enumerate(length):
			gen_session = gen[i]
			ref_session = reference_allvocabs[i]
			for j in range(turn_length):
				self.hyps.append(list(self.dataloader.trim_index(gen_session[j])))
				self.refs.append([list(self.dataloader.trim_index(ref_session[j])[1:])])

	def close(self):
		'''Return a dict which contains:

			* **bleu**: bleu value.
			* **bleu hashvalue**: hash value of reference data.
		'''
		try:
			self.hash_relevant_data(self.refs)
			return {"bleu": \
				corpus_bleu(self.refs, self.hyps, smoothing_function=SmoothingFunction().method7), \
				"bleu hashvalue": self.hashvalue()}
		except ZeroDivisionError as _:
			raise ZeroDivisionError("Bleu smoothing divided by zero. This is a known bug of corpus_bleu, \
				usually caused when there is only one sample and the sample length is 1.")

class SingleTurnDialogRecorder(MetricBase):
	'''A metric-like class for recording generated sentences and references.

	Arguments:
		dataloader (:class:`cotk.GenerationBase`): A language generation dataloader.
		post_allvocabs_key (str): Dialog post are passed to :func:`forward`
			by ``data[post_allvocabs_key]``.
			Default: ``post_allvocabs``.
		resp_allvocabs_key (str): Dialog responses are passed to :func:`forward`
			by ``data[resp_allvocabs_key]``.
			Default: ``resp_allvocabs``.
		gen_key (str): Sentence generated by model are passed to :func:`forward` by
			``data[gen_key]``. Default: ``gen``.
	'''
	def __init__(self, dataloader, post_allvocabs_key="post_allvocabs", \
			resp_allvocabs_key="resp_allvocabs", gen_key="gen"):
		super().__init__()
		self.dataloader = dataloader
		self.post_allvocabs_key = post_allvocabs_key
		self.resp_allvocabs_key = resp_allvocabs_key
		self.gen_key = gen_key
		self.post_list = []
		self.resp_list = []
		self.gen_list = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				* data[post_allvocabs_key] (list or :class:`numpy.ndarray` of `int`):
					Dialog posts with all vocabs.
					Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
					Size: `[batch_size, max_sentence_length]`.
				* data[resp_allvocabs_key] (list or :class:`numpy.ndarray` of `int`):
					Dialog responses with all vocabs.
					Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
					Size: `[batch_size, max_sentence_length]`.
				* data[gen_key] (list or :class:`numpy.ndarray` of `int`):
					Sentences generated by model. Contains end token (eg: ``<eos>``)`,
					but without start token (eg: ``<go>``).
					Size: `[batch_size, gen_sentence_length]`.
		'''
		post_allvocabs = data[self.post_allvocabs_key]
		resp_allvocabs = data[self.resp_allvocabs_key]
		gen = data[self.gen_key]
		if len(post_allvocabs) != len(resp_allvocabs) or len(resp_allvocabs) != len(gen):
			raise ValueError("Batch num is not matched.")
		for i, post_sen in enumerate(post_allvocabs):
			self.post_list.append(self.dataloader.index_to_sen(post_sen[1:]))
			self.resp_list.append(self.dataloader.index_to_sen(resp_allvocabs[i][1:]))
			self.gen_list.append(self.dataloader.index_to_sen(gen[i]))

	def close(self):
		'''Return a dict which contains:

			* **post**: a list of post sentences.
			* **resp**: a list of response sentences.
			* **gen**: a list of generated sentences.
		'''
		return {"post": self.post_list, "resp": self.resp_list, "gen": self.gen_list}

class MultiTurnDialogRecorder(MetricBase):
	'''A metric-like class for recording generated sentences and references.

	Arguments:
		dataloader (:class:`cotk.GenerationBase`): A language generation dataloader.
		context_allvocabs_key (str): Dialog context are passed to :func:`forward` by
			``data[context_key]``. Default: ``context_allvocabs``.
		reference_allvocabs_key (str): Dialog references with all vocabs
			are passed to :func:`forward` by ``data[reference_allvocabs_key]``.
			Default: ``reference_allvocabs``.
		gen_key (str): Sentences generated by model are passed to :func:`forward` by
			``data[gen_key]``. Default: ``gen``.
		turn_len_key (str): Turn length are passed to :func:`forward` by
			``data[turn_len_key]``. Default: ``turn_length``.
	'''
	def __init__(self, dataloader, context_allvocabs_key="context_allvocabs", \
			reference_allvocabs_key="reference_allvocabs", gen_key="gen", \
			turn_len_key="turn_length"):
		super().__init__()
		self.dataloader = dataloader
		self.context_allvocabs_key = context_allvocabs_key
		self.reference_allvocabs_key = reference_allvocabs_key
		self.gen_key = gen_key
		self.turn_len_key = turn_len_key
		self.context_list = []
		self.reference_list = []
		self.gen_list = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				* data[context_allvocabs_key] (list or :class:`numpy.ndarray` of `int`):
					Dialog post.
					A 3-d padded array containing id of words.
					Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
					Size: `[batch_size, _turn_length, max_sentence_length]`.
				* data[reference_allvocabs_key] (list or :class:`numpy.ndarray` of `int`):
					Dialog responses with all vocabs. A 3-d padded array containing id of words.
					Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
					Size: `[batch_size, max_turn_length, max_sentence_length]`.
				* data[gen_key] (list or :class:`numpy.ndarray` of `int`):
					Sentences generated by model.
					A 3-d padded array containing id of words.
					Contains  end token (eg: ``<eos>``), but without start token (eg: ``<go>``).
					Size: `[batch_size, max_turn_length, gen_sentence_length]`.
				* data[turn_len_key] (list or :class:`numpy.ndarray`):
					Length of turns in each sample.
					Size: `[batch_size]`.
		'''
		context_allvocabs = data[self.context_allvocabs_key]
		reference_allvocabs = data[self.reference_allvocabs_key]
		gen = data[self.gen_key]
		turn_length = data[self.turn_len_key]
		if len(gen) != len(reference_allvocabs):
			raise ValueError("Batch num is not matched.")
		for i, context_sen in enumerate(context_allvocabs):
			self.context_list.append(self.dataloader.multi_turn_index_to_sen( \
				np.array(context_sen), ignore_first_token=True))
			self.reference_list.append(self.dataloader.multi_turn_index_to_sen( \
				np.array(reference_allvocabs[i]), turn_length=turn_length[i], ignore_first_token=True))
			self.gen_list.append(self.dataloader.multi_turn_index_to_sen( \
				np.array(gen[i]), turn_length=turn_length[i]))
			print(turn_length[i])
			print(len(self.reference_list[-1]))

			if len(self.reference_list[-1]) != len(self.gen_list[-1]):
				raise ValueError("Reference turn num %d != gen turn num %d." % \
						(len(self.reference_list[-1]), len(self.gen_list[-1])))

	def close(self):
		'''Return a dict which contains:

			* **context**: a list of post sentences.
			* **reference**: a list of response sentences.
			* **gen**: a list of generated sentences.
		'''
		return {"context": self.context_list, "reference": self.reference_list, "gen": self.gen_list}

class LanguageGenerationRecorder(MetricBase):
	'''A metric-like class for recorder BLEU.

	Arguments:
		dataloader (:class:`cotk.GenerationBase`): A language generation dataloader.
		gen_key (str): Sentences generated by model are passed to :func:`forward` by
			``data[gen_key]``. Default: ``gen``.
	'''
	def __init__(self, dataloader, gen_key="gen"):
		super().__init__()
		self.dataloader = dataloader
		self.gen_key = gen_key
		self.gen_list = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				* data[gen_key] (list or :class:`numpy.ndarray` of `int`):
					Sentences generated by model.
					Contains end token (eg: ``<eos>``), but without start token (eg: ``<go>``).
					Size: `[batch_size, gen_sentence_length]`.
		'''
		gen = data[self.gen_key]
		for sen in gen:
			self.gen_list.append(self.dataloader.index_to_sen(sen))

	def close(self):
		'''Return a dict which contains:

			* **gen**: a list of generated sentences.
		'''
		return {"gen": self.gen_list}

class HashValueRecorder(MetricBase):
	'''A metric-like class for recording hash value metric.
	'''
	def __init__(self, hash_key="hashvalue"):
		super().__init__()
		self._hash_key = hash_key
		self.unordered_hash = None

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains hashvalue.
		'''
		if "hashvalue" in data:
			if self.unordered_hash is None:
				self.unordered_hash = UnorderedSha256()
			self.unordered_hash.update_hash(data["hashvalue"])

	def close(self):
		'''Return a dict which contains the items which all the
			metric components returned.
		'''
		if self.unordered_hash:
			return {self._hash_key: self.unordered_hash.digest()}
		else:
			return {}

class MetricChain(MetricBase):
	'''A metric-like class for stacked metric. You can use this class
	making multiples metric combination like one.

	Examples:
		>>> metric = MetricChain()
		>>> metric.add_metric(BleuCorpusMetric())
		>>> metric.add_metric(SingleDialogRecorder(dataloader))
	'''
	def __init__(self):
		super().__init__()
		self.metric_list = []

	def add_metric(self, metric):
		'''Add metric for processing.

		Arguments:
			metric (MetricBase): a metric class.
		'''
		if not isinstance(metric, MetricBase):
			raise TypeError("Metric must be a subclass of MetricBase")
		self.metric_list.append(metric)

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains keys which all the
				metric components need.
		'''
		for metric in self.metric_list:
			metric.forward(data)

	def close(self):
		'''Return a dict which contains the items which all the
			metric components returned.
		'''
		ret_dict = {}
		for metric in self.metric_list:
			ret_dict.update(metric.close())
		return ret_dict
